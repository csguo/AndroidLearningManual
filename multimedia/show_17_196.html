
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML xmlns="http://www.w3.org/1999/xhtml">
<HEAD>
<TITLE>Android Framework下StageFright框架流程解读_Android学习手册</TITLE>
<META content="text/html; charset=gb2312" http-equiv=Content-Type><LINK  rel=stylesheet type=text/css href="../css/c5.css">
<META name=GENERATOR content="MSHTML 8.00.6001.18702">
<meta name="Description" content=Android基础 Android组件 Android用户界面 Android设备功能 Android数据存储 Android网络应用 Android游戏开发 Android多媒体 Android源码开发 Android高级进阶 Android面试题/>
<meta name="Keywords" content="android 学习手册">

</HEAD>
<BODY id=homesecond class=serverscripting>
<DIV id=wrapper>
<DIV id=header><h1><a href="../index.html">Android学习手册</a></h1> 
</DIV>

<DIV id=navfirst>
<div id="indexGuide"><UL><li ><A href="../basic/index.html">Android基础</A> </li><li ><A href="../component/index.html">Android组件</A> </li><li ><A href="../userinterface/index.html">用户界面</A> </li><li ><A href="../device/index.html">设备功能</A> </li><li ><A href="../datastorage/index.html">数据存储</A> </li><li ><A href="../network/index.html">网络应用</A> </li><li ><A href="../game/index.html">游戏开发</A> </li><li class="navcurrentLink"><A href="index.html">多媒体</A> </li><li ><A href="../source/index.html">源码开发</A> </li><li ><A href="../advance/index.html">高级进阶</A> </li><li ><A href="../interview/index.html">Android面试题</A> </li></UL></div>
</DIV>
<DIV id=navsecond>
<DIV id=course>
<div id="kcTitle">课 程 表</div>


  <h2><A title="Android多媒体功能" href="index_196.html">Android多媒体功能</A></h2>
<UL><li ><A title=Android Media架构 href="show_1_196.html">Android Media架构</A> </li><li ><A title=Android 媒体库（一） href="show_2_196.html">Android 媒体库（一）</A> </li><li ><A title=Android 媒体库（二） href="show_3_196.html">Android 媒体库（二）</A> </li><li ><A title=Android 媒体库（三） href="show_4_196.html">Android 媒体库（三）</A> </li><li ><A title=Android 媒体库（四） href="show_5_196.html">Android 媒体库（四）</A> </li><li ><A title=Android 媒体库（五） href="show_6_196.html">Android 媒体库（五）</A> </li><li ><A title=Android 媒体库（六） href="show_7_196.html">Android 媒体库（六）</A> </li><li ><A title=Android 媒体库（七） href="show_8_196.html">Android 媒体库（七）</A> </li><li ><A title=Android 媒体库（八） href="show_9_196.html">Android 媒体库（八）</A> </li><li ><A title=Android OpenMax多媒体引擎（一） href="show_10_196.html">Android OpenMax多媒体引擎（一）</A> </li><li ><A title=Android OpenMax多媒体引擎（二） href="show_11_196.html">Android OpenMax多媒体引擎（二）</A> </li><li ><A title=Android OpenMax多媒体引擎（三） href="show_12_196.html">Android OpenMax多媒体引擎（三）</A> </li><li ><A title=Android 多媒体框架源码全面解析 href="show_13_196.html">Android 多媒体框架源码全面解析</A> </li><li ><A title=Android多媒体模块代码分析 href="show_14_196.html">Android多媒体模块代码分析</A> </li><li ><A title=Android多媒体功能之AudioRecorder音频录制实现，Rexsee源码分享 href="show_15_196.html">Android多媒体功能之AudioRecorder音频录制实现，Rexsee源码分享</A> </li><li ><A title=Android2.X新的多媒体框架Stagefright讨论 href="show_16_196.html">Android2.X新的多媒体框架Stagefright讨论</A> </li><li class="currentLink"><A title=Android Framework下StageFright框架流程解读 href="show_17_196.html">Android Framework下StageFright框架流程解读</A> </li><li ><A title=基于stagefright 的 OMXCodec与OMX事件处理流程 href="show_18_196.html">基于stagefright 的 OMXCodec与OMX事件处理流程</A> </li><li ><A title=Android如何获取多媒体文件信息 href="show_19_196.html">Android如何获取多媒体文件信息</A> </li><li ><A title=通过路径获取媒体文件信息 href="show_20_196.html">通过路径获取媒体文件信息</A> </li></UL>
  <h2><A title="Android多媒体之2D" href="index_197.html">Android多媒体之2D</A></h2>

  <h2><A title="Android多媒体之3D" href="index_198.html">Android多媒体之3D</A></h2>

  <h2><A title="Android多媒体之Bitmap" href="index_199.html">Android多媒体之Bitmap</A></h2>

  <h2><A title="Android多媒体之Canvas" href="index_200.html">Android多媒体之Canvas</A></h2>

  <h2><A title="Android多媒体之Drawable" href="index_201.html">Android多媒体之Drawable</A></h2>

  <h2><A title="Android多媒体之gif" href="index_202.html">Android多媒体之gif</A></h2>

  <h2><A title="Android多媒体之MediaStore" href="index_203.html">Android多媒体之MediaStore</A></h2>

  <h2><A title="Android多媒体之动画" href="index_204.html">Android多媒体之动画</A></h2>

  <h2><A title="Android多媒体之分辨率" href="index_205.html">Android多媒体之分辨率</A></h2>

  <h2><A title="Android多媒体之分屏幕" href="index_206.html">Android多媒体之分屏幕</A></h2>

</DIV></DIV>
<DIV id=maincontent>
<DIV id=w3school>
<H1></H1>
<P><STRONG></STRONG></P></DIV>

<DIV>
<H2>Android Framework下StageFright框架流程解读</H2>
<div style="line-height:20px; font-size:14px;"><p style="color:#3333FF;">1、StageFright介绍</p><p>Android froyo版本多媒体引擎做了变动，新添加了stagefright框架，并且默认情况android选择stagefright，并没有完全抛弃opencore，主要是做了一个OMX层，仅仅是对opencore的omx-component部分做了引用。stagefright是在MediaPlayerService这一层加入的，和opencore是并列的。Stagefright在 Android中是以shared library的形式存在(libstagefright.so)，其中的module -- AwesomePlayer可用来播放video/audio。 AwesomePlayer提供许多API，可以让上层的应用程序(Java/JNI)来调用。</p><p style="color:#3333FF;">2、StageFright数据流封装</p><p>2.1由数据源DataSource生成MediaExtractor。通过MediaExtractor::Create(dataSource)来实现。Create方法通过两步来生成相应的MediaExtractor(MediaExtractor.cpp)：</p><p>通过dataSource-&gt;sniff来探测数据类型</p><p>生成相应的Extractor：</p><p><pre><font class="keyword">if </font>(!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_MPEG4)</p><p>		</font>|| !strcasecmp(mime, <font class="Fields">"audio/mp4"</font>)) {</p><p>	<font class="keyword">return </font><font class="keyword">new </font>MPEG4Extractor(source);</p><p></font>} <font class="keyword">else </font><font class="keyword">if </font>(!strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_MPEG)) {</p><p>	<font class="keyword">return </font><font class="keyword">new </font>MP3Extractor(source, meta);</p><p></font>} <font class="keyword">else </font><font class="keyword">if </font>(!strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_AMR_NB)</p><p>		|| !strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_AMR_WB)) {</p><p>	<font class="keyword">return </font><font class="keyword">new </font>AMRExtractor(source);</p><p></font>} <font class="keyword">else </font><font class="keyword">if </font>(!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_WAV)) {</p><p>	<font class="keyword">return </font><font class="keyword">new </font>WAVExtractor(source);</p><p></font>} <font class="keyword">else </font><font class="keyword">if </font>(!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_OGG)) {</p><p>	<font class="keyword">return </font><font class="keyword">new </font>OggExtractor(source);</p><p></font>} <font class="keyword">else </font><font class="keyword">if </font>(!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_MATROSKA)) {</p><p>	<font class="keyword">return </font><font class="keyword">new </font>MatroskaExtractor(source);</p><p></font>} <font class="keyword">else </font><font class="keyword">if </font>(!strcasecmp(mime, MEDIA_MIMETYPE_CONTAINER_MPEG2TS)) {</p><p>	<font class="keyword">return </font><font class="keyword">new </font>MPEG2TSExtractor(source);</p><p>}</p><p></pre></p><p>2.2 把音视频轨道分离，生成mVideoTrack和mAudioTrack两个MediaSource。代码如下（AwesomePlayer.cpp）：</p><p><pre><font class="keyword">if </font>(!haveVideo && !strncasecmp(mime, <font class="Fields">"video/"</font>, 6)) {</p><p>	setVideoSource(extractor-&gt;getTrack(i));</p><p>	</font>haveVideo = <font class="keyword">true</font>;</p><p></font>} <font class="keyword">else </font><font class="keyword">if </font>(!haveAudio && !strncasecmp(mime, <font class="Fields">"audio/"</font>, 6)) {</p><p>	setAudioSource(extractor-&gt;getTrack(i));</p><p>	</font>haveAudio = <font class="keyword">true</font>;</p><p>}</p><p></pre></p><p>2.3 得到的这两个MediaSource，只具有parser功能，没有decode功能。还需要对这两个MediaSource做进一步的包装，获取了两个MediaSource（具有parse和decode功能）：</p><p><pre>mVideoSource = OMXCodec::Create(</p><p>	mClient.interface(), mVideoTrack-&gt;getFormat(),</p><p>	<font class="keyword">false</font>, <font class="Comments">// createEncoder</font></p><p>	mVideoTrack,</p><p>	NULL, flags);</p><p>mAudioSource = OMXCodec::Create(</p><p>	mClient.interface(), mAudioTrack-&gt;getFormat(),</p><p>	<font class="keyword">false</font>, <font class="Comments">// createEncoder</font></p><p>	mAudioTrack);</p><p></pre></p><p>当调用MediaSource.start()方法后，它的内部就会开始从数据源获取数据并解析，等到缓冲区满后便停止。在AwesomePlayer里就可以调用MediaSource的read方法读取解码后的数据。</p><p>对于mVideoSource来说，读取的数据：mVideoSource-&gt;read(&mVideoBuffer, &options)交给显示模块进行渲染，mVideoRenderer-&gt;render(mVideoBuffer);</p><p>对mAudioSource来说，用mAudioPlayer对mAudioSource进行封装，然后由mAudioPlayer负责读取数据和播放控制。</p><p style="color:#3333FF;">3、StageFright的Decode</p><p>经过“数据流的封装”得到的两个MediaSource，其实是两个OMXCodec。AwesomePlayer和mAudioPlayer都是从MediaSource中得到数据进行播放。AwesomePlayer得到的是最终需要渲染的原始视频数据，而mAudioPlayer读取的是最终需要播放的原始音频数据。也就是说，从OMXCodec中读到的数据已经是原始数据了。</p><p>OMXCodec是怎么把数据源经过parse、decode两步以后转化成原始数据的。从OMXCodec::Create这个构造方法开始，它的参数：</p><p>IOMX &omx指的是一个OMXNodeInstance对象的实例。</p><p>MetaData ＆meta这个参数由MediaSource.getFormat获取得到。这个对象的主要成员就是一个KeyedVector&lt;uint32_t, typed_data&gt; mItems，里面存放了一些代表MediaSource格式信息的名值对。</p><p>bool createEncoder指明这个OMXCodec是编码还是解码。</p><p>MediaSource ＆source是一个MediaExtractor。</p><p>char *matchComponentName指定一种Codec用于生成这个OMXCodec。</p><p>先使用findMatchingCodecs寻找对应的Codec，找到以后为当前IOMX分配节点并注册事件监听器：omx-&gt;allocateNode(componentName, observer, &node)。最后，把IOMX封装进一个OMXCodec：</p><p><pre>sp&lt;OMXCodec&gt; codec = <font class="keyword">new </font>OMXCodec(</p><p>	omx, node, quirks,</p><p>	createEncoder, mime, componentName,</p><p>	source);</p><p></pre></p><p>这样就得到了OMXCodec。</p><p>AwesomePlayer中得到这个OMXCodec后，首先调用mVideoSource-&gt;start()进行初始化。 OMXCodec初始化主要是做两件事：</p><p>向OpenMAX发送开始命令。mOMX-&gt;sendCommand(mNode, OMX_CommandStateSet, OMX_StateIdle)</p><p>调用allocateBuffers()分配两个缓冲区，存放在Vector&lt;BufferInfo&gt; mPortBuffers[2]中，分别用于输入和输出。</p><p>AwesomePlayer开始播放后，通过mVideoSource-&gt;read(&mVideoBuffer, &options)读取数据。mVideoSource-&gt;read(&mVideoBuffer, &options)具体是调用OMXCodec.read来读取数据。而OMXCodec.read主要分两步来实现数据的读取：</p><p>通过调用drainInputBuffers()对mPortBuffers[kPortIndexInput]进行填充，这一步完成parse。由OpenMAX从数据源把demux后的数据读取到输入缓冲区，作为OpenMAX的输入。</p><p>通过fillOutputBuffers()对mPortBuffers[kPortIndexOutput]进行填充，这一步完成decode。由OpenMAX对输入缓冲区中的数据进行解码，然后把解码后可以显示的视频数据输出到输出缓冲区。</p><p>AwesomePlayer通过mVideoRenderer-&gt;render(mVideoBuffer)对经过parse和decode处理的数据进行渲染。一个mVideoRenderer其实就是一个包装了IOMXRenderer的AwesomeRemoteRenderer：</p><p><pre>mVideoRenderer = <font class="keyword">new </font>AwesomeRemoteRenderer(</p><p>	mClient.interface()-&gt;createRenderer(</p><p>	mISurface, component,</p><p>	(OMX_COLOR_FORMATTYPE)format,</p><p>	decodedWidth, decodedHeight,</p><p>	mVideoWidth, mVideoHeight,</p><p>	rotationDegrees));</p><p></pre></p><p style="color:#3333FF;">4、StageFright处理流程</p><p>Audioplayer为AwesomePlayer的成员，audioplayer通过callback来驱动数据的获取，awesomeplayer则是通过videoevent来驱动。二者有个共性，就是数据的获取都抽象成mSource-&gt;Read()来完成，且read内部把parser和dec 绑在一起。Stagefright AV同步部分，audio完全是callback驱动数据流，video部分在onVideoEvent里会获取audio的时间戳，是传统的AV时间戳做同步。</p><p>4.1 AwesomePlayer的Video主要有以下几个成员：</p><p><pre>mVideoSource(解码视频)</p><p>mVideoTrack(从多媒体文件中读取视频数据)</p><p>mVideoRenderer(对解码好的视频进行格式转换，android使用的格式为RGB565)</p><p>mISurface(重绘图层)</p><p>mQueue(event事件队列)</p><p></pre></p><p>4.2 stagefright运行时的Audio部分抽象流程如下：</p><p>设置mUri的路径</p><p>启动mQueue，创建一个线程来运行 threadEntry（命名为TimedEventQueue，这个线程就是event调度器）。</p><p>打开mUri所指定的文件的头部，则会根据类型选择不同的分离器（如MPEG4Extractor）。</p><p>使用MPEG4Extractor对MP4进行音视频轨道的分离，并返回MPEG4Source类型的视频轨道给mVideoTrack</p><p>根据mVideoTrack中的编码类型来选择解码器，avc的编码类型会选择AVCDecoder，并返回给mVideoSource，并设置mVideoSource中的mSource为mVideoTrack。</p><p>插入onVideoEvent到Queue中，开始解码播放。</p><p>通过mVideoSource对象来读取解析好的视频buffer。</p><p>如果解析好的buffer还没到AV时间戳同步的时刻，则推迟到下一轮操作。</p><p>mVideoRenderer为空，则进行初始化（如果不使用 OMX会将mVideoRenderer设置为AwesomeLocalRenderer）。</p><p>通过mVideoRenderer对象将解析好的视频buffer转换成RGB565格式，并发给display模块进行图像绘制。</p><p>将onVideoEvent重新插入event调度器来循环。</p><p>4.3 数据由源到最终解码后的流程如下：</p><p><pre>URI,FD</p><p>｜</p><p>	DataSource</p><p>	｜</p><p>	MediaExtractor</p><p>		|</p><p>		mVideoTrack   mAudioTrack//音视频数据流</p><p>		｜</p><p>		mVideoSource   mAudioSource//音视频解码器</p><p>			|</p><p>		  mVideoBuffermAudioPlayer</p><p></pre></p><p>说明：</p><p>设置DataSource，数据源可以两种URI和FD。URI可以http://，rtsp://等。FD是一个本地文件描述符，能过FD，可以找到对应的文件。</p><p>由DataSource生成MediaExtractor。通过sp&lt;MediaExtractor&gt; extractor = MediaExtractor::Create(dataSource);来实现。 MediaExtractor::Create(dataSource)会根据不同的数据内容创建不同的数据读取对象。</p><p>通过调用setVideoSource由MediaExtractor分解生成音频数据流（mAudioTrack）和视频数据流（mVideoTrack）。</p><p>onPrepareAsyncEvent()如果DataSource是URL的话，根据地址获取数据，并开始缓冲，直到获取到mVideoTrack和mAudioTrack。mVideoTrack和mAudioTrack通过调用initVideoDecoder()和initAudioDecoder()来生成mVideoSource和mAudioSource这两个音视频解码器。然后调用postBufferingEvent_l()提交事件开启缓冲。</p><p>数据缓冲的执行函数是onBufferingUpdate()。缓冲区有足够的数据可以播放时，调用play_l()开始播放。play_l()中关键是调用了postVideoEvent_l()，提交了mVideoEvent。这个事件执行时会调用函数onVideoEvent()。这个函数通过调用mVideoSource-&gt;read(&mVideoBuffer,&options)进行视频解码。音频解码通过mAudioPlayer实现。</p><p>视频解码器解码后通过mVideoSource-&gt;read读取一帧帧的数据，放到mVideoBuffer中，最后通过mVideoRenderer-&gt;render(mVideoBuffer)把视频数据发送到显示模块。当需要暂停或停止时，调用cancelPlayerEvents来提交事</div>

</DIV></DIV>

<DIV id=footer style="display:none">
<P align="center">  
 
 
 
    </P>
</DIV></BODY></HTML>

